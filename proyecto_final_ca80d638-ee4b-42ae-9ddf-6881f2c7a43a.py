# Databricks notebook source
# MAGIC %md
# MAGIC # Regresi√≥n lineal: predecir los gastos m√©dicos de pacientes
# MAGIC ---

# COMMAND ----------

# MAGIC %md
# MAGIC Para este ejercicio utilizaremos los datos presentados en [este](https://www.kaggle.com/mirichoi0218/insurance) dataset de Kaggle en el cual se presentan datos de seguros m√©dicos. 
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC ## Descarga e instalaci√≥n de librer√≠as

# COMMAND ----------

# MAGIC %md
# MAGIC Lo primero que se har√° es descargar la librer√≠a **[regressors](https://pypi.org/project/regressors/)** que ayudar√° a hacer un an√°lisis m√°s profundo sobre la regresi√≥n lineal.

# COMMAND ----------

import pandas as pd
import seaborn as sns
sns.set(style='whitegrid', context='notebook')

# COMMAND ----------

# MAGIC %md
# MAGIC ## Descargando los datos
# MAGIC Descarguemos los datos y veamos c√≥mo se ven.

# COMMAND ----------

# MAGIC %fs ls /FileStore/tables/
# MAGIC
# MAGIC
# MAGIC
# MAGIC

# COMMAND ----------

df_spk = spark.read.csv("dbfs:/FileStore/tables/insurance.csv", header=True, inferSchema=True)
df_spk.show()


# COMMAND ----------

df = df_spk.toPandas()
df.head()


# COMMAND ----------

# MAGIC %md
# MAGIC ## Analizando los datos
# MAGIC Se observar√° c√≥mo se distribuyen los datos de la variable a predecir.

# COMMAND ----------

print(df.shape)
df.charges.hist(bins = 40)

# COMMAND ----------

# MAGIC %md
# MAGIC Algo que analizar, seg√∫n este gr√°fico, es entender qu√© est√° pasando con los datos arriba de los 50,000. Parece haber muy pocos datos de este lado.

# COMMAND ----------

df[df.charges>50000]
df = df[df.charges<50000]

# COMMAND ----------

# MAGIC %md
# MAGIC En este caso, al ser pocos datos (6 de 1338), eliminaremos estos datos at√≠picos. A modo did√°ctico producen m√°s ruido en la predicci√≥n que se est√° intentando hacer en este ejercicio. 
# MAGIC
# MAGIC Sin embargo es importante aclarar que **NO SE DEBEN ELIMINAR** datos at√≠picos sin antes conocer a alguien que conozca o sea experto en los datos para que pueda guiarnos mejor sobre ellos.

# COMMAND ----------

# MAGIC %md
# MAGIC ## Viendo correlaciones
# MAGIC Ahora entendamos nuestros datos viendo c√≥mo se distribuyen y correlacionan. 

# COMMAND ----------

import matplotlib.pyplot as plt
sns.pairplot(df, height=2.5)
plt.show()

# COMMAND ----------

# MAGIC %md
# MAGIC Puntos interesantes a ver:
# MAGIC
# MAGIC - Hay 3 grupos de personas diferentes que se clasifican en edad / cargos, esto puede ser un punto a analizar despu√©s.
# MAGIC
# MAGIC En general los valores se distribuyen de manera esperada. Con valores extremos en el caso de los cargos, sin embargo esto es de esperarse pues los cargos en los hospitales pueden variar mucho por quedarse un d√≠a m√°s en el hospital o incluso por procedimientos extras.
# MAGIC
# MAGIC - Parece que los datos est√°n limpios, la variable de √≠ndice de masa corporal se distribuye de manera normal o gausiana, lo cual ser√≠a esperado en un √≠ndice de este tipo.

# COMMAND ----------

import numpy as np
numeric_cols = ['age', 'bmi', 'children', 'charges']
cm = np.corrcoef(df[numeric_cols].values.T)
sns.set(font_scale=1.5)
sns.heatmap(cm,annot=True, yticklabels=numeric_cols,xticklabels=numeric_cols)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Utilizando las dem√°s variables
# MAGIC Las dem√°s variables son variables categoricas, sexo, fumador, regi√≥n. Para poder utilizarlas utilizaremos la funci√≥n **[get_dummies](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)** de pandas. 
# MAGIC
# MAGIC Ahora la ver√°s en acci√≥n

# COMMAND ----------

df = pd.get_dummies(df, columns=['sex','smoker','region'], drop_first=True)
df.head()

# COMMAND ----------

# MAGIC %md
# MAGIC ## Creando modelos 
# MAGIC Primero se usar√° un modelo con todas las variables.

# COMMAND ----------

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# COMMAND ----------

X_cols = list(set(df.columns)-set(['charges']))
y_col = ['charges']

X = df[X_cols].values
y = df[y_col].values

X_train, X_test, y_train, y_test = train_test_split(X,y)
sc_x = StandardScaler().fit(X)
sc_y = StandardScaler().fit(y)

X_train = sc_x.transform(X_train)
X_test = sc_x.transform(X_test)
y_train = sc_y.transform(y_train)
y_test = sc_y.transform(y_test)

model = LinearRegression()
model.fit(X_train,y_train)
y_pred = model.predict(X_test)

# COMMAND ----------

y_pred.shape

# COMMAND ----------

# MAGIC %md
# MAGIC ## Funciones de m√©tricas
# MAGIC El siguiente punto es calcular las m√©tricas del modelo.

# COMMAND ----------

import sklearn.metrics as metrics
mse = metrics.mean_squared_error(y_test,y_pred)
r2 = metrics.r2_score(y_test, y_pred)

print("r2 ", r2.round(4))
print("mse: ", mse.round(4))

# COMMAND ----------

# MAGIC %md
# MAGIC El siguiente c√≥digo muestra un resumen general de los resultados.

# COMMAND ----------

# Asegurarse de que ambos arrays est√°n aplanados (1D)
residuals = y_test.ravel() - y_pred.ravel()

# COMMAND ----------

# Ver los primeros 5 residuales
print("Primeros residuales:", residuals[:5])

# COMMAND ----------

import matplotlib.pyplot as plt

plt.scatter(y_pred, residuals, alpha=0.6)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel("Predicciones (y_pred)")
plt.ylabel("Residuales (y_test - y_pred)")
plt.title("Gr√°fico de Residuales")
plt.grid(True)
plt.show()

# COMMAND ----------

print("Residuales promedio:", residuals.mean().round(4))  # Cerca de 0 en muchos casos
print("RMSE desde residuales:", np.sqrt((residuals**2).mean()).round(4))  # Igual al RMSE

# COMMAND ----------

# MAGIC %md
# MAGIC Finalmente tenemos la funci√≥n que calcula los residuales. Es importante notar que es una simple resta entre los valores reales y los predichos.

# COMMAND ----------

# MAGIC %md
# MAGIC ## Segundo modelo
# MAGIC Estos resultados de arriba son buenos, pero se pueden mejorar. Intentaremos hacer algunas transformaciones sobre las variables que pueden ser de utilidad.

# COMMAND ----------

df_second = df.copy()
df_second['age2'] = df_second.age**2
df_second['sobrepeso'] = (df_second.bmi >= 30).astype(int)
df_second['sobrepeso*fumador'] = df_second.sobrepeso * df_second.smoker_yes

# COMMAND ----------

# MAGIC %md
# MAGIC Analizando el segundo modelo

# COMMAND ----------

X_cols = ['sobrepeso*fumador', 'smoker_yes', 'age2', 'children']
y_col = ['charges']

X = df_second[X_cols].values
y = df_second[y_col].values

X_train, X_test, y_train, y_test = train_test_split(X,y)
sc_x = StandardScaler().fit(X)
sc_y = StandardScaler().fit(y)

X_train = sc_x.transform(X_train)
X_test = sc_x.transform(X_test)
y_train = sc_y.transform(y_train)
y_test = sc_y.transform(y_test)

model = LinearRegression(fit_intercept=False)
model.fit(X_train,y_train)
y_pred = model.predict(X_test)

# COMMAND ----------

mse = metrics.mean_squared_error(y_test,y_pred)
r2 = metrics.r2_score(y_test, y_pred)

print("r2 ", r2.round(4))
print("mse: ", mse.round(4))

# COMMAND ----------

residuals = y_test.ravel() - y_pred.ravel()

plt.scatter(y_pred, residuals, alpha=0.6)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel("Predicciones (y_pred)")
plt.ylabel("Residuales")
plt.title("Gr√°fico de Residuales - Modelo Mejorado")
plt.grid(True)
plt.show()

# COMMAND ----------

coeficientes = model.coef_.flatten()
for col, coef in zip(X_cols, coeficientes):
    print(f"{col}: {coef:.4f}")

# COMMAND ----------

# MAGIC %md
# MAGIC Interpretaci√≥n variable por variable:
# MAGIC
# MAGIC ‚∏ª
# MAGIC
# MAGIC üîπ sobrepeso*fumador: 0.4940
# MAGIC 	‚Ä¢	Este coeficiente indica que un paciente que es fumador y tiene sobrepeso ver√° un incremento de casi medio desv√≠o est√°ndar en sus gastos m√©dicos esperados.
# MAGIC 	‚Ä¢	Es el factor m√°s influyente en este modelo.
# MAGIC 	‚Ä¢	Representa un efecto sin√©rgico: cuando se combinan dos factores de riesgo, el impacto es mayor que la suma de sus partes.
# MAGIC
# MAGIC üìò Interpretaci√≥n pr√°ctica:
# MAGIC Fumar y tener sobrepeso juntos elevan considerablemente los costos m√©dicos, mucho m√°s que cada uno por separado.
# MAGIC
# MAGIC ‚∏ª
# MAGIC
# MAGIC üîπ smoker_yes: 0.4653
# MAGIC 	‚Ä¢	Fumar por s√≠ solo tambi√©n tiene un efecto fuerte y positivo en los gastos m√©dicos.
# MAGIC 	‚Ä¢	Este valor es alto, y confirma lo que se sabe cl√≠nicamente: el tabaquismo es un predictor fuerte y directo de aumento en costos m√©dicos.
# MAGIC
# MAGIC üìò Interpretaci√≥n pr√°ctica:
# MAGIC Un paciente fumador, a√∫n sin sobrepeso, tiene una fuerte probabilidad de incurrir en mayores costos.
# MAGIC
# MAGIC ‚∏ª
# MAGIC
# MAGIC üîπ age2: 0.3195
# MAGIC 	‚Ä¢	Este t√©rmino cuadr√°tico de la edad permite capturar una relaci√≥n no lineal creciente.
# MAGIC 	‚Ä¢	A diferencia de un modelo con solo ‚Äúedad‚Äù, este refleja que los costos no crecen de forma constante, sino que aceleran con la edad.
# MAGIC
# MAGIC üìò Interpretaci√≥n pr√°ctica:
# MAGIC A medida que envejecemos, los gastos m√©dicos tienden a crecer m√°s r√°pidamente, especialmente en edades mayores.
# MAGIC
# MAGIC ‚∏ª
# MAGIC
# MAGIC üîπ children: 0.0591
# MAGIC 	‚Ä¢	Es el coeficiente m√°s bajo del modelo.
# MAGIC 	‚Ä¢	Sugerir√≠a una relaci√≥n leve y posiblemente indirecta entre la cantidad de hijos y el gasto m√©dico.
# MAGIC
# MAGIC üìò Interpretaci√≥n pr√°ctica:
# MAGIC Tener m√°s hijos puede asociarse a mayores necesidades m√©dicas, pero su impacto directo es relativamente peque√±o.

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC
# MAGIC üßæ  Informe Final y Conclusiones del Proyecto
# MAGIC
# MAGIC ‚∏ª
# MAGIC
# MAGIC ü©∫ T√≠tulo del Proyecto:
# MAGIC
# MAGIC Predicci√≥n de Gastos M√©dicos Usando Regresi√≥n Lineal con Ingenier√≠a de Caracter√≠sticas
# MAGIC
# MAGIC ‚∏ª
# MAGIC
# MAGIC üéØ Objetivo General:
# MAGIC
# MAGIC Predecir los gastos m√©dicos de pacientes a partir de variables como edad, √≠ndice de masa corporal (IMC), tabaquismo, n√∫mero de hijos y regi√≥n. Para ello se utiliz√≥ un modelo de regresi√≥n lineal, complementado con t√©cnicas de transformaci√≥n y an√°lisis de datos.
# MAGIC
# MAGIC ‚∏ª
# MAGIC
# MAGIC üìö Resumen de los pasos realizados:
# MAGIC
# MAGIC Paso	Descripci√≥n
# MAGIC 1	Carga de datos con Spark y conversi√≥n a Pandas.
# MAGIC 2	An√°lisis exploratorio inicial y limpieza de valores extremos.
# MAGIC 3	Visualizaci√≥n de relaciones entre variables con pairplot y heatmap.
# MAGIC 4	Codificaci√≥n de variables categ√≥ricas con get_dummies.
# MAGIC 5	Entrenamiento del modelo de regresi√≥n lineal con datos estandarizados.
# MAGIC 6	Evaluaci√≥n inicial del modelo usando R¬≤ y MSE.
# MAGIC 7	An√°lisis de residuales del modelo base.
# MAGIC 8	Ingenier√≠a de caracter√≠sticas: age¬≤, variable binaria de sobrepeso, e interacci√≥n sobrepeso*fumador.
# MAGIC 9	Entrenamiento del modelo mejorado.
# MAGIC 10	Evaluaci√≥n del nuevo modelo con m√©tricas y residuales.
# MAGIC 11	Interpretaci√≥n de coeficientes del modelo mejorado.
# MAGIC 12	Informe final y conclusiones.
# MAGIC
# MAGIC
# MAGIC ‚∏ª
# MAGIC
# MAGIC üìà Resultados comparativos entre modelos:
# MAGIC
# MAGIC M√©trica	Modelo Base	Modelo Mejorado
# MAGIC R¬≤	~0.74	0.832
# MAGIC MSE	~0.55	0.1618
# MAGIC Residuales	Curvatura visible y heterocedasticidad	M√°s centrados, menor dispersi√≥n, sin patr√≥n evidente
# MAGIC
# MAGIC ‚úÖ El modelo mejorado super√≥ claramente al modelo base tanto en precisi√≥n como en ajuste a los datos.
# MAGIC
# MAGIC ‚∏ª
# MAGIC
# MAGIC üìå Variables m√°s influyentes (modelo mejorado):
# MAGIC
# MAGIC Variable	Coeficiente	Impacto
# MAGIC sobrepeso*fumador	0.4940	Muy alto
# MAGIC smoker_yes	0.4653	Alto
# MAGIC age2	0.3195	Moderado-alto
# MAGIC children	0.0591	Bajo
# MAGIC
# MAGIC
# MAGIC ‚∏ª
# MAGIC
# MAGIC üß† Principales aprendizajes del proyecto:
# MAGIC 	‚Ä¢	El an√°lisis de residuales es clave para detectar errores sistem√°ticos en el modelo.
# MAGIC 	‚Ä¢	La ingenier√≠a de caracter√≠sticas (como incluir interacciones y transformaciones cuadr√°ticas) puede mejorar significativamente un modelo simple.
# MAGIC 	‚Ä¢	La regresi√≥n lineal sigue siendo una herramienta poderosa y explicativa si se prepara bien el dataset.
# MAGIC 	‚Ä¢	La combinaci√≥n de visualizaciones, m√©tricas y coeficientes ofrece una visi√≥n integral y transparente del modelo.
# MAGIC
# MAGIC ‚∏ª
# MAGIC
# MAGIC üõ†Ô∏è Posibles mejoras futuras:
# MAGIC 	‚Ä¢	Aplicar modelos no lineales como regresi√≥n polin√≥mica, Random Forest o XGBoost.
# MAGIC 	‚Ä¢	Usar regularizaci√≥n (Ridge, Lasso) para evitar sobreajuste y mejorar la interpretabilidad.
# MAGIC 	‚Ä¢	Probar con m√°s datos o variables (por ejemplo, historial m√©dico, nivel socioecon√≥mico, h√°bitos de ejercicio).
# MAGIC 	‚Ä¢	Validaci√≥n cruzada para mejorar la robustez del modelo.
# MAGIC
# MAGIC ‚∏ª
# MAGIC
# MAGIC üéì Conclusi√≥n Final:
# MAGIC
# MAGIC Este proyecto demostr√≥ c√≥mo pasar de un enfoque b√°sico de predicci√≥n a uno m√°s profundo, en el que se combinan t√©cnicas estad√≠sticas, visuales y conceptuales para construir un modelo mejorado, explicable y √∫til. Aprendiste a:
# MAGIC 	‚Ä¢	Explorar y entender los datos.
# MAGIC 	‚Ä¢	Modelar relaciones complejas entre variables.
# MAGIC 	‚Ä¢	Evaluar y mejorar un modelo basado en evidencia.
# MAGIC 	‚Ä¢	Comunicar hallazgos con claridad y solidez.
# MAGIC
# MAGIC ‚ú® Este no es solo un proyecto de datos, es una demostraci√≥n de pensamiento anal√≠tico, interpretaci√≥n estad√≠stica y buenas pr√°cticas en ciencia de datos.
# MAGIC
# MAGIC